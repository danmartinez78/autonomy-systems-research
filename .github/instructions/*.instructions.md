# PR Review Instructions (Copilot / LLM Agents)

This repository is a public knowledge base for robotics and autonomy research. It includes reading notes, syntheses, journal entries, and evergreen “knowledge base” pages. Because LLM agents may author or edit content here, PR review MUST prioritize **technical accuracy**, **verifiable sourcing**, and **anti-hallucination safeguards** over style or verbosity.

> Principle: If a claim cannot be verified from the cited source or is not clearly framed as opinion/speculation, it should not be merged as fact.

---

## 1) Review Goals

When reviewing a PR, you are responsible for checking:

1. **Technical correctness**
   - Claims reflect established robotics/autonomy knowledge and do not contain obvious conceptual errors.
   - Methods, metrics, and terminology are used correctly.
   - Safety- and deployment-relevant statements are conservative and properly qualified.

2. **Reference validity**
   - Linked sources exist, match the cited claims, and are not misquoted or misrepresented.
   - Paper titles/authors/years/venues are accurate (no invented citations).
   - DOIs/arXiv IDs/URLs are real and correspond to the described work.

3. **Hallucination resistance**
   - Identify any content that appears invented, overly confident, or suspiciously specific without support.
   - Ensure uncertain material is labeled as uncertain and grounded with references.

4. **Repository conventions**
   - Front matter is present and correct.
   - Tags are sensible and consistent.
   - Internal links resolve under GitHub Pages.

---

## 2) Hard Rules (Block Merge If Violated)

Block the PR (request changes) if any of the following are true:

- **Uncited factual claims** are presented as true, especially:
  - performance numbers, benchmarks, SOTA statements
  - safety claims (“this is safe”, “guarantees”, “provably”)
  - algorithmic guarantees, complexity, convergence claims
  - historical claims (“paper X introduced Y”, “industry uses Z”)
- **References appear hallucinated**:
  - dead links, wrong authors/years, non-existent DOI/arXiv
  - paper title doesn’t match content
- **Misrepresentation**:
  - a source is cited but does not actually support the claim
  - a citation is used as decoration rather than evidence
- **Fabricated specificity**:
  - exact values, thresholds, or “standard practice” presented without a source
- **Safety-critical advice** is too casual:
  - missing disclaimers or appropriate caution for real-world autonomy systems

If unsure, default to caution: request changes.

---

## 3) Anti-Hallucination Checklist (Use This Every Time)

### A) Suspicion triggers
Treat these as high-risk for hallucination and verify aggressively:

- “Studies show…”, “It is well known…”, “standard in industry…”
- precise metrics without context (mAP, F1, RMSE, drift, latency) but no setup
- claims of provable optimality, guaranteed safety, or universal superiority
- detailed pipeline steps that read plausible but lack citations
- citations that look formatted but are not easily verifiable

### B) Required actions
For any suspicious claim:

- Ask: “What source supports this exact statement?”
- If no source exists, require:
  - either removal,
  - or rewrite as speculation + rationale,
  - or add a reputable citation with a link that actually works.

### C) “Quarantine phrasing” for uncertain material
If content is not verifiable, it must be rewritten using language like:

- “Hypothesis: …”
- “Working theory: …”
- “Speculation: …”
- “Anecdotally / in our experience…”
- “Open question: …”
- “Needs citation / verification: …”

---

## 4) Reference Verification Standards

When a PR cites a paper/article/report:

1. **The link must resolve** (arXiv, DOI, publisher page, or stable PDF).
2. **Metadata must match**:
   - title, authors, and year match the linked document.
3. **Claims must match the source**:
   - if summarizing, the summary must reflect the source’s actual content.
   - if quoting, quote exactly and attribute properly.
4. **No invented benchmarks**:
   - if the PR compares methods, it must cite the benchmark and conditions.

Recommended citation fields for reading notes:
- Title
- Authors
- Year
- Venue (if known)
- Link (arXiv/DOI/publisher)
- Optional: BibTeX snippet

---

## 5) Technical Review Guidance (Robotics & Autonomy)

When reviewing robotics/autonomy content, check for common failure modes:

### A) Terminology sanity checks
- Localization vs mapping vs SLAM used correctly
- state estimation vs filtering vs smoothing not conflated
- planning vs control vs prediction boundaries are clear
- “behavior trees” vs “FSM” distinctions used accurately
- uncertainty is treated properly (not hand-waved)

### B) Metrics and claims
- metrics are defined (what is measured, on what dataset, under what conditions)
- comparisons are apples-to-apples (same sensors, same environment assumptions)
- avoid “SOTA” unless supported by a current benchmark citation

### C) Safety and deployment caution
- mention that real-world autonomy requires validation, testing, and safety processes
- no claims implying suitability for safety-critical deployment unless explicitly supported

---

## 6) Content-Type Specific Review

### Reading Notes
Must include:
- accurate citation metadata (title/authors/link)
- summary that matches the source
- key claims + evidence assessment (what supports them?)
- limitations / assumptions
- open questions

### Syntheses
Must:
- clearly differentiate:
  - what is known (with citations)
  - what is believed / hypothesized
  - what is unknown / open
- include a “source map” (links to reading notes and/or primary sources)
- avoid false certainty

### Knowledge Base Pages
Must:
- be practical and generalizable
- include “when to use / when not to use”
- call out pitfalls and failure modes
- cite sources for nontrivial claims

### Journal Entries
Can be informal, but:
- must not introduce new “facts” without citations
- should label speculation clearly

---

## 7) Repo Hygiene Checks

Confirm:
- YAML front matter present (title, tags, date/date_read/last_updated)
- tags are lowercase, hyphenated, and consistent
- internal links use `{{ site.baseurl }}` / `relative_url` where appropriate
- generated tag pages are not manually edited (they are regenerated)

If tags/content changed:
- ensure tag pages are regenerated via:
  - `python3 generate-tags.py`
- ensure generated files are committed.

---

## 8) How to Provide Review Feedback (Format)

When requesting changes, be explicit and actionable:

- **Issue:** quote the exact sentence/claim
- **Reason:** (unsupported / incorrect / misleading / unsafe)
- **Fix:** (add citation + link, rewrite as hypothesis, or remove)
- **Verification:** (what to check / how to confirm)

Example:

> **Issue:** “Method X guarantees convergence in dynamic environments.”  
> **Reason:** “Guarantee” is a strong claim; no citation provided.  
> **Fix:** Add a citation that explicitly proves this claim, or rewrite as “often converges under assumptions A/B.”  
> **Verification:** Provide DOI/arXiv link and quote the theorem/assumptions.

---

## 9) Default Stance

This knowledge base is only useful if it is trustworthy.

When in doubt:
- request changes,
- require citations,
- downgrade to uncertainty language,
- or remove claims that can’t be verified.

Be conservative. Accuracy beats completeness.
